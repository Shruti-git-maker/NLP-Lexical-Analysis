{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 17 Nov 2024\n",
        "# **Natural Language Processing**\n",
        "\n",
        "e.g. un-break-able ::\n",
        "prefix - root - suffix : **morphemes**[atoms]\n",
        "\n",
        "\n",
        "e.g. I like to read.::\n",
        "I - Like - To - Read - . every single word is a **token[**molecules]\n",
        "\n",
        "e.g. HE IS RUNNING.::\n",
        "Running has a task attached to it and run is a verb : it is **lexeme**[individual compound]\n",
        "\n",
        "all this together is called **lexical analysis**\n",
        "\n",
        "**Pattern** :\n",
        "e.g. mobile no. 123455678\n",
        "'\\d+'\n",
        "\n"
      ],
      "metadata": {
        "id": "iV0Fqtjl98mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical Analysis :breaking down text into its basic units\n",
        "# steps:\n",
        "# step 1: Toknization\n",
        "# step 2: Stop word removal\n",
        "# Step 4: Stemming and lemmatization\n",
        "# step 5: Advanced canonalization\n",
        "# step 6: Bag Of Words (BOW)\n",
        "# step 7: TF-IDF\n"
      ],
      "metadata": {
        "id": "K4jdhOP__TWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenisation - NLTK method/Regular Expression"
      ],
      "metadata": {
        "id": "j3kHzsVjCAZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbHB0qdX97RW",
        "outputId": "d5e42069-ac96-4eef-e5a3-ecd2f72acf06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches in machine learning and deep learning.\n",
        "\n",
        "Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation.\n",
        "\n",
        "History\"\"\""
      ],
      "metadata": {
        "id": "391xpDLE99zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwi8yqrX992U",
        "outputId": "69751df8-65bc-4b63-850a-8f1c18eb0dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'computer', 'science', 'and', 'especially', 'artificial', 'intelligence', '.', 'It', 'is', 'primarily', 'concerned', 'with', 'providing', 'computers', 'with', 'the', 'ability', 'to', 'process', 'data', 'encoded', 'in', 'natural', 'language', 'and', 'is', 'thus', 'closely', 'related', 'to', 'information', 'retrieval', ',', 'knowledge', 'representation', 'and', 'computational', 'linguistics', ',', 'a', 'subfield', 'of', 'linguistics', '.', 'Typically', 'data', 'is', 'collected', 'in', 'text', 'corpora', ',', 'using', 'either', 'rule-based', ',', 'statistical', 'or', 'neural-based', 'approaches', 'in', 'machine', 'learning', 'and', 'deep', 'learning', '.', 'Major', 'tasks', 'in', 'natural', 'language', 'processing', 'are', 'speech', 'recognition', ',', 'text', 'classification', ',', 'natural-language', 'understanding', ',', 'and', 'natural-language', 'generation', '.', 'History']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regular expression\n",
        "[Regex](https://www.bing.com/images/search?view=detailV2&ccid=%2FDysIXfn&id=C19E12BD370C50B86A558CB726F08067C01CD6AF&thid=OIP._DysIXfnc0vkg9KzXi1tFwHaEt&mediaurl=https%3A%2F%2Ftec-refresh.com%2Fhs-fs%2Fhubfs%2FImported_Blog_Media%2Fregex-cheatsheet.jpg%3Fwidth%3D886%26height%3D563%26name%3Dregex-cheatsheet.jpg&exph=563&expw=886&q=regex%3A+tac-refresh+for+beginners&simid=608037352835922489&FORM=IRPRST&ck=0AEC2626B5D9F72DD079DDC9324BABE3&selectedIndex=4&itb=0&cw=582&ch=501&ajaxhist=0&ajaxserp=0)"
      ],
      "metadata": {
        "id": "vwh2bEUQE6Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "print(tokens)\n",
        "# unable to parse special characters as of now"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKbpZZvh9945",
        "outputId": "20e285c7-c320-48a4-c88b-e75b895c310d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', 'NLP', 'is', 'a', 'subfield', 'of', 'computer', 'science', 'and', 'especially', 'artificial', 'intelligence', 'It', 'is', 'primarily', 'concerned', 'with', 'providing', 'computers', 'with', 'the', 'ability', 'to', 'process', 'data', 'encoded', 'in', 'natural', 'language', 'and', 'is', 'thus', 'closely', 'related', 'to', 'information', 'retrieval', 'knowledge', 'representation', 'and', 'computational', 'linguistics', 'a', 'subfield', 'of', 'linguistics', 'Typically', 'data', 'is', 'collected', 'in', 'text', 'corpora', 'using', 'either', 'rule', 'based', 'statistical', 'or', 'neural', 'based', 'approaches', 'in', 'machine', 'learning', 'and', 'deep', 'learning', 'Major', 'tasks', 'in', 'natural', 'language', 'processing', 'are', 'speech', 'recognition', 'text', 'classification', 'natural', 'language', 'understanding', 'and', 'natural', 'language', 'generation', 'History']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "frequency of words"
      ],
      "metadata": {
        "id": "IXHIQcTLHJSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for every token find the frequency\n",
        "from collections import Counter\n",
        "freq = Counter(tokens)\n",
        "for i in freq.most_common(10):\n",
        "  print(i)\n",
        "# zipps power law: high frequency less importance, such words are stop words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xfRpfrY998Z",
        "outputId": "d299ae72-a07c-4791-aabf-b901164029e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('language', 5)\n",
            "('and', 5)\n",
            "('is', 4)\n",
            "('in', 4)\n",
            "('natural', 4)\n",
            "('processing', 2)\n",
            "('a', 2)\n",
            "('subfield', 2)\n",
            "('of', 2)\n",
            "('with', 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "REmoval f stop words"
      ],
      "metadata": {
        "id": "q6AzKsB2IAZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY5wW9vq979l",
        "outputId": "b6f76c89-ca8d-4830-e8bf-a7d09a1b774e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'have', 'we', 'has', 'there', 'only', \"isn't\", 'down', 'once', 'she', 'same', 'all', 'hers', 'under', 'mustn', \"mustn't\", 'does', 'hasn', 'can', 'any', 'with', 'my', 'off', 'then', 'having', 'than', 'do', 'that', 'did', 'his', \"needn't\", 'y', 'on', 'by', 'this', 'for', 'further', \"that'll\", 'theirs', \"shouldn't\", 'doesn', 'itself', 'wasn', \"you've\", 'below', 'up', \"she's\", 'couldn', 'shouldn', 'here', \"hadn't\", 'against', 'won', 'had', 'or', 'as', 'most', 'no', 'those', \"didn't\", 'being', \"it's\", 'both', 'shan', 'you', 'again', 'some', 'until', \"you'll\", 'when', \"don't\", 'because', 'myself', 'more', 'me', 'whom', 'which', 'hadn', 'are', 'where', 're', 'very', 'be', 'll', 'and', 'through', 'its', \"you're\", 'at', 'why', 'am', 'own', 'will', \"you'd\", 'during', 'it', \"should've\", 'himself', 'the', 'about', 'a', \"couldn't\", \"shan't\", 'just', 'above', 's', 'while', \"doesn't\", \"wasn't\", 'they', 'nor', 'not', 'o', 'should', 'didn', 'what', \"wouldn't\", 'i', 'ma', 'if', 'each', 'few', 'herself', 'an', 't', 'him', 'wouldn', 'mightn', \"won't\", 'is', 'from', \"aren't\", 'don', 'in', 'weren', 'such', 'them', 'doing', 'her', 'been', 'into', 'your', 'ain', 'to', \"mightn't\", 'd', 'how', 'were', 'before', 'out', \"hasn't\", 'after', 'needn', 'but', 'yourself', 'now', 'yourselves', 'ours', 'aren', 'too', 'who', 'between', 'their', \"weren't\", 'ourselves', 'of', 'yours', 'over', 'our', 'themselves', 'other', 'these', 'he', 'was', 'isn', 'so', 'm', 'haven', 've', \"haven't\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words.add('natural')"
      ],
      "metadata": {
        "id": "mZzxLogx98A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmblMBNYIdqu",
        "outputId": "26f95be1-84f4-4732-bc3b-cff3ebbc089a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', 'NLP', 'subfield', 'computer', 'science', 'especially', 'artificial', 'intelligence', 'primarily', 'concerned', 'providing', 'computers', 'ability', 'process', 'data', 'encoded', 'natural', 'language', 'thus', 'closely', 'related', 'information', 'retrieval', 'knowledge', 'representation', 'computational', 'linguistics', 'subfield', 'linguistics', 'Typically', 'data', 'collected', 'text', 'corpora', 'using', 'either', 'rule', 'based', 'statistical', 'neural', 'based', 'approaches', 'machine', 'learning', 'deep', 'learning', 'Major', 'tasks', 'natural', 'language', 'processing', 'speech', 'recognition', 'text', 'classification', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'History']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuation marks or special characters\n",
        "filtered_tokens = [word for word in filtered_tokens if word.isalnum()]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7wfI5--IduS",
        "outputId": "c1b2f265-0cee-4548-8dac-14a0adfa9ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', 'NLP', 'subfield', 'computer', 'science', 'especially', 'artificial', 'intelligence', 'primarily', 'concerned', 'providing', 'computers', 'ability', 'process', 'data', 'encoded', 'natural', 'language', 'thus', 'closely', 'related', 'information', 'retrieval', 'knowledge', 'representation', 'computational', 'linguistics', 'subfield', 'linguistics', 'Typically', 'data', 'collected', 'text', 'corpora', 'using', 'either', 'rule', 'based', 'statistical', 'neural', 'based', 'approaches', 'machine', 'learning', 'deep', 'learning', 'Major', 'tasks', 'natural', 'language', 'processing', 'speech', 'recognition', 'text', 'classification', 'natural', 'language', 'understanding', 'natural', 'language', 'generation', 'History']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming and lemmatization"
      ],
      "metadata": {
        "id": "aTmon_EtI8S_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "for i in stemmed_tokens:\n",
        "  print(i)\n",
        "# only used for those cases where we are sure that these are part of a speech"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT35CLYaIdya",
        "outputId": "4be12968-f9d3-48dc-e147-3f3d538356af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natur\n",
            "languag\n",
            "process\n",
            "nlp\n",
            "subfield\n",
            "comput\n",
            "scienc\n",
            "especi\n",
            "artifici\n",
            "intellig\n",
            "primarili\n",
            "concern\n",
            "provid\n",
            "comput\n",
            "abil\n",
            "process\n",
            "data\n",
            "encod\n",
            "natur\n",
            "languag\n",
            "thu\n",
            "close\n",
            "relat\n",
            "inform\n",
            "retriev\n",
            "knowledg\n",
            "represent\n",
            "comput\n",
            "linguist\n",
            "subfield\n",
            "linguist\n",
            "typic\n",
            "data\n",
            "collect\n",
            "text\n",
            "corpora\n",
            "use\n",
            "either\n",
            "rule\n",
            "base\n",
            "statist\n",
            "neural\n",
            "base\n",
            "approach\n",
            "machin\n",
            "learn\n",
            "deep\n",
            "learn\n",
            "major\n",
            "task\n",
            "natur\n",
            "languag\n",
            "process\n",
            "speech\n",
            "recognit\n",
            "text\n",
            "classif\n",
            "natur\n",
            "languag\n",
            "understand\n",
            "natur\n",
            "languag\n",
            "gener\n",
            "histori\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatising\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "for i in lemmatized_tokens:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGaICGUNId0-",
        "outputId": "ee66fbe8-2355-4275-dbee-0dc699dee0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural\n",
            "language\n",
            "processing\n",
            "NLP\n",
            "subfield\n",
            "computer\n",
            "science\n",
            "especially\n",
            "artificial\n",
            "intelligence\n",
            "primarily\n",
            "concerned\n",
            "providing\n",
            "computer\n",
            "ability\n",
            "process\n",
            "data\n",
            "encoded\n",
            "natural\n",
            "language\n",
            "thus\n",
            "closely\n",
            "related\n",
            "information\n",
            "retrieval\n",
            "knowledge\n",
            "representation\n",
            "computational\n",
            "linguistics\n",
            "subfield\n",
            "linguistics\n",
            "Typically\n",
            "data\n",
            "collected\n",
            "text\n",
            "corpus\n",
            "using\n",
            "either\n",
            "rule\n",
            "based\n",
            "statistical\n",
            "neural\n",
            "based\n",
            "approach\n",
            "machine\n",
            "learning\n",
            "deep\n",
            "learning\n",
            "Major\n",
            "task\n",
            "natural\n",
            "language\n",
            "processing\n",
            "speech\n",
            "recognition\n",
            "text\n",
            "classification\n",
            "natural\n",
            "language\n",
            "understanding\n",
            "natural\n",
            "language\n",
            "generation\n",
            "History\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# e.g. lemmatizer\n",
        "print(\"tokens\",lemmatizer.lemmatize(\"tokens\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEJPcG60Id3b",
        "outputId": "f26a8928-fa25-406c-db79-17859607efc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens token\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import pos\n",
        "print(\"jumping\",lemmatizer.lemmatize(\"jumping\",pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mss7nx4xId6A",
        "outputId": "3bc4ed0d-fdfa-4b82-8ede-70ff54e78ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jumping jump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# combine stemming and lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "stemmed_tokens = [stemmer.stem(lemmatized_tokens) for lemmatized_tokens in lemmatized_tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK3ca4JlMfT-",
        "outputId": "bceeed62-66bc-4515-8c46-dc1ead3c8f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natur', 'languag', 'process', 'nlp', 'is', 'a', 'subfield', 'of', 'comput', 'scienc', 'and', 'especi', 'artifici', 'intellig', 'it', 'is', 'primarili', 'concern', 'with', 'provid', 'comput', 'with', 'the', 'abil', 'to', 'process', 'data', 'encod', 'in', 'natur', 'languag', 'and', 'is', 'thu', 'close', 'relat', 'to', 'inform', 'retriev', 'knowledg', 'represent', 'and', 'comput', 'linguist', 'a', 'subfield', 'of', 'linguist', 'typic', 'data', 'is', 'collect', 'in', 'text', 'corpu', 'use', 'either', 'rule', 'base', 'statist', 'or', 'neural', 'base', 'approach', 'in', 'machin', 'learn', 'and', 'deep', 'learn', 'major', 'task', 'in', 'natur', 'languag', 'process', 'are', 'speech', 'recognit', 'text', 'classif', 'natur', 'languag', 'understand', 'and', 'natur', 'languag', 'gener', 'histori']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag-of-words"
      ],
      "metadata": {
        "id": "EtlFP1FIOyGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bow model\n",
        "def bag_of_words(tokens):\n",
        "  word_freq = {}\n",
        "  for token in tokens:\n",
        "    if token not in word_freq:\n",
        "      word_freq[token] = 1\n",
        "    else:\n",
        "      word_freq[token] += 1\n",
        "  return word_freq\n",
        "\n",
        "bow = bag_of_words(lemmatized_tokens)\n",
        "for i in bow.items():\n",
        "  print(i)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW5pOuOXMfQj",
        "outputId": "3d4b26ef-4cc9-4ca9-8dd6-fa906cfe019c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Natural', 1)\n",
            "('language', 5)\n",
            "('processing', 2)\n",
            "('NLP', 1)\n",
            "('is', 4)\n",
            "('a', 2)\n",
            "('subfield', 2)\n",
            "('of', 2)\n",
            "('computer', 2)\n",
            "('science', 1)\n",
            "('and', 5)\n",
            "('especially', 1)\n",
            "('artificial', 1)\n",
            "('intelligence', 1)\n",
            "('It', 1)\n",
            "('primarily', 1)\n",
            "('concerned', 1)\n",
            "('with', 2)\n",
            "('providing', 1)\n",
            "('the', 1)\n",
            "('ability', 1)\n",
            "('to', 2)\n",
            "('process', 1)\n",
            "('data', 2)\n",
            "('encoded', 1)\n",
            "('in', 4)\n",
            "('natural', 4)\n",
            "('thus', 1)\n",
            "('closely', 1)\n",
            "('related', 1)\n",
            "('information', 1)\n",
            "('retrieval', 1)\n",
            "('knowledge', 1)\n",
            "('representation', 1)\n",
            "('computational', 1)\n",
            "('linguistics', 2)\n",
            "('Typically', 1)\n",
            "('collected', 1)\n",
            "('text', 2)\n",
            "('corpus', 1)\n",
            "('using', 1)\n",
            "('either', 1)\n",
            "('rule', 1)\n",
            "('based', 2)\n",
            "('statistical', 1)\n",
            "('or', 1)\n",
            "('neural', 1)\n",
            "('approach', 1)\n",
            "('machine', 1)\n",
            "('learning', 2)\n",
            "('deep', 1)\n",
            "('Major', 1)\n",
            "('task', 1)\n",
            "('are', 1)\n",
            "('speech', 1)\n",
            "('recognition', 1)\n",
            "('classification', 1)\n",
            "('understanding', 1)\n",
            "('generation', 1)\n",
            "('History', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GxSu7LzVMfOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlNvMF7LMfMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-mtEI0N1MfJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s7Tz1x5tId9d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}